{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetFeatures=['tavg','tmin','tmax','prcp','snow','wspd','pres']\n",
    "input_size = len(targetFeatures) \n",
    "hidden_size = 64      # Number of hidden units in LSTM\n",
    "num_layers = 2        # Number of LSTM layers\n",
    "output_size =  len(targetFeatures) \n",
    "seq_length = 30       # Sequence length (number of past days to consider)\n",
    "#testing webhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(WeatherLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        \n",
    "        # LSTM Layer forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # lstm_out shape: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Check if lstm_out has 3 dimensions or just 2\n",
    "        if lstm_out.dim() == 3:\n",
    "            # If lstm_out has 3 dimensions, we access the last time step\n",
    "            out = lstm_out[:, -1, :]  # Get the output from the last time step for each batch\n",
    "        else:\n",
    "            # If lstm_out has 2 dimensions (batch_size, hidden_size), just use it\n",
    "            out = lstm_out\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time    0\n",
      "tavg    0\n",
      "tmin    0\n",
      "tmax    0\n",
      "prcp    0\n",
      "snow    0\n",
      "wspd    0\n",
      "pres    0\n",
      "dtype: int64\n",
      "time    0\n",
      "tavg    0\n",
      "tmin    0\n",
      "tmax    0\n",
      "prcp    0\n",
      "snow    0\n",
      "wspd    0\n",
      "pres    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('newYork.csv')\n",
    "df.drop(['wpgt','tsun','wdir'],axis=1,inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "df.replace([float('inf'), -float('inf')], 0, inplace=True)\n",
    "print(df.isnull().sum())  # To check for any NaN values\n",
    "print((df == float('inf')).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import *\n",
    "# Assuming your data is in the form of pandas DataFrame `df` and your features and targets are already separated\n",
    "# Let's say the columns are 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres'\n",
    "\n",
    "# Features and targets\n",
    "features = df[targetFeatures].values\n",
    "targets = df[targetFeatures].values  # Same columns for now, could be adjusted\n",
    "\n",
    "# Split the data into training and validation sets (e.g., 80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_train_reshaped = X_train_tensor.view(-1, X_train_tensor.size(-1))\n",
    "X_val_reshaped = X_val_tensor.view(-1, X_val_tensor.size(-1))\n",
    "\n",
    "# 2. Initialize MinMaxScaler and apply it\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "\n",
    "# 3. Reshape back to 3D (batch_size, sequence_length, features)\n",
    "X_train_scaled = torch.tensor(X_train_scaled).view(X_train_tensor.shape)\n",
    "X_val_scaled = torch.tensor(X_val_scaled).view(X_val_tensor.shape)\n",
    "\n",
    "# Convert scaled data back to tensors\n",
    "X_train_scaled = X_train_scaled.float()\n",
    "X_val_scaled = X_val_scaled.float()\n",
    "\n",
    "# 4. Create TensorDataset and DataLoader\n",
    "y_train_tensor = y_train_tensor.float()  # Make sure target tensors are float as well\n",
    "y_val_tensor = y_val_tensor.float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_scaled, y_val_tensor)\n",
    "\n",
    "batch_size = 32  # Set your batch size as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27263/376631979.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/home/arjun/.envs/ml/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27263/376631979.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/arjun/.envs/ml/lib/python3.11/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Avg Train Loss: 78.3053\n",
      "Epoch [2/20], Avg Train Loss: 42.9739\n",
      "Epoch [3/20], Avg Train Loss: 38.1773\n",
      "Epoch [4/20], Avg Train Loss: 9.9060\n",
      "Epoch [5/20], Avg Train Loss: 7.1556\n",
      "Epoch [6/20], Avg Train Loss: 6.4154\n",
      "Epoch [7/20], Avg Train Loss: 4.7449\n",
      "Epoch [8/20], Avg Train Loss: 4.2897\n",
      "Epoch [9/20], Avg Train Loss: 4.0630\n",
      "Epoch [10/20], Avg Train Loss: 3.7462\n",
      "Epoch [11/20], Avg Train Loss: 3.6965\n",
      "Epoch [12/20], Avg Train Loss: 3.6140\n",
      "Epoch [13/20], Avg Train Loss: 3.5794\n",
      "Epoch [14/20], Avg Train Loss: 3.5959\n",
      "Epoch [15/20], Avg Train Loss: 3.5643\n",
      "Epoch [16/20], Avg Train Loss: 3.5622\n",
      "Epoch [17/20], Avg Train Loss: 3.5635\n",
      "Epoch [18/20], Avg Train Loss: 3.5606\n",
      "Epoch [19/20], Avg Train Loss: 3.5617\n",
      "Epoch [20/20], Avg Train Loss: 3.5663\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you're using a device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = WeatherLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=1, alpha=0.9, weight_decay=0.001)\n",
    "from torch_optimizer import Lookahead\n",
    "base_optimizer =optim.RMSprop(model.parameters(), lr=1, alpha=0.1, weight_decay=0.0001)\n",
    "optimizer = Lookahead(base_optimizer, k=15, alpha=1)\n",
    "\n",
    "\n",
    "# Reduce learning rate\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "# Enable mixed precision if you're using GPU\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Sample training loop with optimizations\n",
    "num_epochs = 20 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (seq, target) in enumerate(train_loader):\n",
    "        seq, target = seq.to(device), target.to(device)\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            output = model(seq)\n",
    "            loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights with gradient scaling\n",
    "        if (i + 1) % 4 == 0:  # Gradient accumulation every 4 mini-batches\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print training loss every 100 iterations (optional)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_train_loss = train_loss / (i + 1)\n",
    "\n",
    "    # Average training loss for the entire epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0105\n",
      "RMSE: 20.9647\n",
      "MAE: 2.0174\n",
      "R²: 0.6361\n",
      "Explained Variance Score: 0.6438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the evaluation function with additional metrics\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for seq, target in val_loader:\n",
    "            # Move data to the device (GPU or CPU)\n",
    "            seq, target = seq.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(seq)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and true labels for metrics\n",
    "            all_preds.append(output.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "            all_labels.append(target.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)  # Mean Absolute Error\n",
    "    r2 = r2_score(all_labels, all_preds)  # R-squared score\n",
    "    evs = explained_variance_score(all_labels, all_preds)  # Explained Variance Score\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Average validation loss\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"Explained Variance Score: {evs:.4f}\")\n",
    "\n",
    "    return avg_val_loss, rmse, mae, r2, evs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `val_loader` is your validation DataLoader\n",
    "# `model` is the trained model, `criterion` is the loss function (MSELoss)\n",
    "# `device` is your computation device (GPU or CPU)\n",
    "val_loss, rmse, mae, r2, evs = evaluate_model(model, val_loader, criterion, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save('model_scripted.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
